{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import shutil\n",
    "from typing import List\n",
    "\n",
    "import torch\n",
    "from dataset import AMRDataSetFast, DataCollatorForSeq2Seq\n",
    "from model_utils import (activate_embeds, assert_all_frozen, freeze_embeds,\n",
    "                         freeze_params, get_ETMG2graph,\n",
    "                         get_inverse_sqrt_schedule_with_warmup, get_MTEG2text,\n",
    "                         get_MTMG2partial, get_MTMG2TG, get_PTPG2partial)\n",
    "from run_multitask_unified_pretraining import smart_emb_init\n",
    "from spring_amr.tokenization_bart import PENMANBartTokenizer\n",
    "from torch.optim import AdamW\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import AutoConfig, AutoModelForMaskedLM\n",
    "\n",
    "os.environ[\"TOKENIZERS_PARALLELISM\"] = \"true\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = {\n",
    "    'model_name_or_path': 'vinai/bartpho-syllable',\n",
    "    'output_dir': '/AIHCM/KGQA/NLPCore/graph2text/AMRBART/output/pre-train/tmt',\n",
    "    'train_file': '/AIHCM/KGQA/NLPCore/graph2text/data/tmt/all_v2_p1.jsonl',\n",
    "    'val_file': '/AIHCM/KGQA/NLPCore/graph2text/data/tmt/all_v2_p1.jsonl',\n",
    "    'test_file': '/AIHCM/KGQA/NLPCore/graph2text/data/tmt/all_v2_p2.jsonl',\n",
    "    'block_size': 256,                  # Optional input sequence length after tokenization.\n",
    "    'smart_init': False,                # Initializing AMR Vocab according to similar tokens\n",
    "    'max_steps': 100000,                # If > 0: set total number of training steps to perform. Override num_train_epochs\n",
    "    'gradient_accumulation_steps': 1,   # Number of updates steps to accumulate before performing a backward/update pass\n",
    "    'weight_decay': 0.0,                # Weight decay if we apply some\n",
    "    'learning_rate': 5e-5,              # Initial learning rate for AdamW\n",
    "    'adam_epsilon': 1e-8,               # Epsilon for AdamW optimizer\n",
    "    'warmup_steps': 2500,               # Linear warmup over warmup_steps\n",
    "    'mlm_amr': True,                    # * [Empty text + Masked Graph -> Graph]\n",
    "    'mlm_text': True,                   # * [Masked Text + Empty Graph -> Text]\n",
    "    'mlm_text_plus_amr': True,          # * [Masked text + Graph -> Text] (apply dynamic masking rate)\n",
    "    'mlm_amr_plus_text': False,         # [Text + Masked Graph -> Graph] (apply dynamic masking rate)\n",
    "    'mlm_joint_to_amr': False,          # [Masked Text + Masked Graph -> Graph]\n",
    "    'mlm_joint_to_text': False,         # [Masked Text + Masked Graph -> Text]\n",
    "    'mlm_joint_to_joint': False,\n",
    "    'joint_train_interval': 1,          # The interval of joint AMR and text training\n",
    "    'max_grad_norm': 1.0,               # Max gradient norm\n",
    "    'logging_steps': 1000,              # Log every X updates steps\n",
    "    'evaluate_during_training': True,\n",
    "    'freeze_embeds': False,\n",
    "    'freeze_encoder': False,\n",
    "    'freeze_decoder': False,\n",
    "    'save_total_limit': None,\n",
    "    # CUDA\n",
    "    'no_cuda': True,\n",
    "    'cuda_index': 1,\n",
    "    'fp16': False,                      # Whether to use 16-bit (mixed) precision (through NVIDIA apex) instead of 32-bit\n",
    "    'fp16_opt_level': 'O1',             # For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3']\n",
    "    'per_gpu_train_batch_size': 1,      # Batch size per GPU/CPU for training\n",
    "    'per_gpu_eval_batch_size': 1,       # Batch size per GPU/CPU for evaluation\n",
    "}\n",
    "t_total = args['max_steps']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['no_cuda']:\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() and not args['no_cuda'] else \"cpu\")\n",
    "    args['n_gpu'] = 0 if args['no_cuda'] else torch.cuda.device_count()\n",
    "else:\n",
    "    torch.cuda.set_device(args['cuda_index'])\n",
    "    device = torch.device(\"cuda\", args['cuda_index'])\n",
    "    args['n_gpu'] = torch.cuda.device_count()\n",
    "args['device'] = device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['device']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = PENMANBartTokenizer.from_pretrained(\n",
    "#     args['model_name_or_path'], \n",
    "#     collapse_name_ops=False, \n",
    "#     use_pointer_tokens=True, \n",
    "#     raw_graph=False,\n",
    "# )\n",
    "import pickle\n",
    "with open('/AIHCM/KGQA/NLPCore/graph2text/models/vinai/bartpho-syllable/tokenizer.pkl', 'rb') as f:\n",
    "    tokenizer = pickle.load(f)\n",
    "\n",
    "if args['block_size'] <= 0:\n",
    "    args['block_size'] = tokenizer.model_max_length\n",
    "    # Our input block size will be the max possible for the model\n",
    "else:\n",
    "    args['block_size'] = min(args['block_size'], tokenizer.model_max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "config = AutoConfig.from_pretrained(args['model_name_or_path'], cache_dir=None)\n",
    "model = AutoModelForMaskedLM.from_pretrained(\n",
    "    args['model_name_or_path'],\n",
    "    from_tf=bool(\".ckpt\" in args['model_name_or_path']),\n",
    "    config=config,\n",
    "    cache_dir=None,\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "if args['freeze_encoder']:  # 151m\n",
    "    freeze_params(model.get_encoder())\n",
    "    assert_all_frozen(model.get_encoder())\n",
    "\n",
    "if args['freeze_decoder']:  # 201m\n",
    "    freeze_params(model.get_decoder())\n",
    "    assert_all_frozen(model.get_decoder())\n",
    "    \n",
    "if args['freeze_embeds']: # 40m\n",
    "    freeze_embeds(model)\n",
    "else:\n",
    "    activate_embeds(model)\n",
    "\n",
    "model.to(args['device'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "515"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_params = [\n",
    "    n for n, p in model.named_parameters() if p.requires_grad\n",
    "]\n",
    "len(train_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "396830720"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(p.numel() for p in model.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "AMRDataset = AMRDataSetFast(\n",
    "    tokenizer=tokenizer,\n",
    "    train_file=args['train_file'],\n",
    "    validation_file=args['val_file'],\n",
    "    test_file=args['test_file'],\n",
    "    pad_to_max_length=False,\n",
    "    max_src_length=args['block_size'],\n",
    "    max_tgt_length=256,\n",
    ")\n",
    "AMRDataset.setup()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples:  8178\n",
      "dev samples:  8178\n"
     ]
    }
   ],
   "source": [
    "# Dummy Test\n",
    "train_dataset = AMRDataset.train_dataset\n",
    "dev_dataset = AMRDataset.valid_dataset\n",
    "print('train samples: ', len(train_dataset))\n",
    "print('dev samples: ', len(dev_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq2seq_collate_fn = DataCollatorForSeq2Seq(\n",
    "    tokenizer,\n",
    "    model=model,\n",
    "    label_pad_token_id=-100,\n",
    "    pad_to_multiple_of=None\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args['smart_init']:\n",
    "    smart_emb_init(tokenizer, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['train_batch_size'] = args['per_gpu_train_batch_size'] * max(1, args['n_gpu'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = RandomSampler(train_dataset)\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    sampler=train_sampler,\n",
    "    batch_size=args['train_batch_size'],\n",
    "    collate_fn=seq2seq_collate_fn,\n",
    "    num_workers=6,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_train_epochs = args['max_steps'] // (len(train_dataloader) // args['gradient_accumulation_steps']) + 1\n",
    "num_train_epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "no_decay = [\"bias\", \"LayerNorm.weight\"]\n",
    "optimizer_grouped_parameters = [\n",
    "    {\n",
    "        \"params\": [\n",
    "            p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)\n",
    "        ],\n",
    "        \"weight_decay\": args['weight_decay'],\n",
    "    },\n",
    "    {\n",
    "        \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n",
    "        \"weight_decay\": 0.0,\n",
    "    },\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = AdamW(optimizer_grouped_parameters, lr=args['learning_rate'], eps=args['adam_epsilon'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "scheduler = get_inverse_sqrt_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=args['warmup_steps'], num_training_steps=t_total\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "args['fp16'] = False\n",
    "if args['fp16']:\n",
    "    from apex import amp\n",
    "    model, optimizer = amp.initialize(model, optimizer, opt_level=args['fp16_opt_level'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "args['n_gpu']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-gpu training (should be after apex fp16 initialization)\n",
    "if args['n_gpu'] > 1 and args['fp16']:\n",
    "    model = torch.nn.DataParallel(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ids_to_clean_text(tokenizer, generated_ids: List[int]):\n",
    "    generated_ids.masked_fill_(generated_ids == -100, tokenizer.pad_token_id)\n",
    "    # gen_text = tokenizer.batch_decode(generated_ids, clean_up_tokenization_spaces=False)\n",
    "    gen_text = tokenizer.convert_ids_to_tokens(generated_ids)\n",
    "    return \" \".join(gen_text)\n",
    "\n",
    "def save_dummy_batch2(output_dir, input_ids, dec_inp_ids, labels, tokenizer, prefix=\"train\"):\n",
    "    dummy_ids, dummy_tokens = [], []\n",
    "    for idx in range(len(input_ids)):\n",
    "        ith_dict, ith_tok_dict = {}, {}\n",
    "        ith_dict[\"input_ids\"] = input_ids[idx].tolist()\n",
    "        ith_dict[\"label_ids\"] = labels[idx].tolist()\n",
    "        ith_dict[\"dec_inp_ids\"] = dec_inp_ids[idx].tolist()\n",
    "        dummy_ids.append(ith_dict)\n",
    "\n",
    "        ith_tok_dict[\"input_tokens\"] = ids_to_clean_text(tokenizer, input_ids[idx])\n",
    "        ith_tok_dict[\"label_tokens\"] = ids_to_clean_text(tokenizer, labels[idx])\n",
    "        ith_tok_dict[\"dec_inp_tokens\"] = ids_to_clean_text(tokenizer, dec_inp_ids[idx])\n",
    "        dummy_tokens.append(ith_tok_dict)\n",
    "\n",
    "    with open(output_dir + f\"/dummy_{prefix}_ids.json\", \"w\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(dummy_ids, fout, indent=4, ensure_ascii=False)\n",
    "    with open(output_dir + f\"/dummy_{prefix}_token.json\", \"w\", encoding=\"utf-8\") as fout:\n",
    "        json.dump(dummy_tokens, fout, indent=4, ensure_ascii=False)\n",
    "\n",
    "def _sorted_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False):\n",
    "    ordering_and_checkpoint_path = []\n",
    "\n",
    "    glob_checkpoints = glob.glob(os.path.join(args['output_dir'], \"{}-*\".format(checkpoint_prefix)))\n",
    "\n",
    "    for path in glob_checkpoints:\n",
    "        if use_mtime:\n",
    "            ordering_and_checkpoint_path.append((os.path.getmtime(path), path))\n",
    "        else:\n",
    "            regex_match = re.match(\".*{}-([0-9]+)\".format(checkpoint_prefix), path)\n",
    "            if regex_match and regex_match.groups():\n",
    "                ordering_and_checkpoint_path.append((int(regex_match.groups()[0]), path))\n",
    "\n",
    "    checkpoints_sorted = sorted(ordering_and_checkpoint_path)\n",
    "    checkpoints_sorted = [checkpoint[1] for checkpoint in checkpoints_sorted]\n",
    "    return checkpoints_sorted\n",
    "\n",
    "\n",
    "def _rotate_checkpoints(args, checkpoint_prefix=\"checkpoint\", use_mtime=False):\n",
    "    if not args['save_total_limit']:\n",
    "        return\n",
    "    if args['save_total_limit'] <= 0:\n",
    "        return\n",
    "\n",
    "    # Check if we should delete older checkpoint(s)\n",
    "    checkpoints_sorted = _sorted_checkpoints(args, checkpoint_prefix, use_mtime)\n",
    "    if len(checkpoints_sorted) <= args['save_total_limit']:\n",
    "        return\n",
    "\n",
    "    number_of_checkpoints_to_delete = max(0, len(checkpoints_sorted) - args['save_total_limit'])\n",
    "    checkpoints_to_be_deleted = checkpoints_sorted[:number_of_checkpoints_to_delete]\n",
    "    for checkpoint in checkpoints_to_be_deleted:\n",
    "        print(\"Deleting older checkpoint [{}] due to args['save_total_limit']\".format(checkpoint))\n",
    "        shutil.rmtree(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(args, eval_dataset, collate_fn, model, tokenizer, prefix=\"\"):\n",
    "    # Loop to handle MNLI double evaluation (matched, mis-matched)\n",
    "    eval_output_dir = args['output_dir']\n",
    "\n",
    "    os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "    args['eval_batch_size'] = args['per_gpu_eval_batch_size'] * max(1, args['n_gpu'])\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        sampler=eval_sampler,\n",
    "        batch_size=args['eval_batch_size'],\n",
    "        collate_fn=collate_fn,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    # if args.n_gpu > 1:\n",
    "    #     model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Eval!\n",
    "    print(\"***** Running evaluation *****\")\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "\n",
    "    pbar = tqdm(eval_dataloader, desc=\"Evaluating\")\n",
    "    for batch in pbar:\n",
    "\n",
    "        with torch.no_grad():\n",
    "            if args['mlm_amr']:\n",
    "                masked_input, attention_mask, dec_input, labels = get_ETMG2graph(batch, tokenizer, mlm_prob=0.35)\n",
    "                masked_input = masked_input.to(args['device'])\n",
    "                labels = labels.to(args['device'])\n",
    "                dec_input = dec_input.to(args['device'])\n",
    "                outputs = model(\n",
    "                    input_ids=masked_input,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=dec_input,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                amr_loss = outputs[0]\n",
    "            else:\n",
    "                amr_loss = 0\n",
    "\n",
    "            if args['mlm_text']:\n",
    "                masked_input, attention_mask, dec_input, labels = get_MTEG2text(batch, tokenizer, mlm_prob=0.35)\n",
    "                masked_input = masked_input.to(args['device'])\n",
    "                labels = labels.to(args['device'])\n",
    "                dec_input = dec_input.to(args['device'])\n",
    "                outputs = model(\n",
    "                    input_ids=masked_input,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=dec_input,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                text_loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "            else:\n",
    "                text_loss = 0\n",
    "\n",
    "            if args['mlm_text_plus_amr']:\n",
    "                masked_input, attention_mask, dec_input, labels = get_PTPG2partial(batch, tokenizer, inp=\"text\")\n",
    "                masked_input = masked_input.to(args['device'])\n",
    "                labels = labels.to(args['device'])\n",
    "                dec_input = dec_input.to(args['device'])\n",
    "                outputs = model(\n",
    "                    input_ids=masked_input,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=dec_input,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                text_joint_loss = outputs[0]\n",
    "            else:\n",
    "                text_joint_loss = 0\n",
    "\n",
    "            if args['mlm_amr_plus_text']:\n",
    "                masked_input, attention_mask, dec_input, labels = get_PTPG2partial(batch, tokenizer, inp=\"amr\")\n",
    "                masked_input = masked_input.to(args['device'])\n",
    "                labels = labels.to(args['device'])\n",
    "                dec_input = dec_input.to(args['device'])\n",
    "                outputs = model(\n",
    "                    input_ids=masked_input,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=dec_input,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                amr_joint_loss = outputs[0]\n",
    "            else:\n",
    "                amr_joint_loss = 0\n",
    "\n",
    "            if args['mlm_joint_to_text']:\n",
    "                mlm_prob = 0.35\n",
    "                masked_input, attention_mask, dec_input, labels = get_MTMG2partial(batch, tokenizer, inp=\"text\", mlm_prob=mlm_prob)\n",
    "                masked_input = masked_input.to(args['device'])\n",
    "                labels = labels.to(args['device'])\n",
    "                dec_input = dec_input.to(args['device'])\n",
    "                outputs = model(\n",
    "                    input_ids=masked_input,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=dec_input,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                text_joint_loss2 = outputs[0]\n",
    "            else:\n",
    "                text_joint_loss2 = 0\n",
    "\n",
    "            if args['mlm_joint_to_amr']:\n",
    "                mlm_prob = 0.35\n",
    "                masked_input, attention_mask, dec_input, labels = get_MTMG2partial(batch, tokenizer, inp=\"amr\", mlm_prob=mlm_prob)\n",
    "                masked_input = masked_input.to(args['device'])\n",
    "                labels = labels.to(args['device'])\n",
    "                dec_input = dec_input.to(args['device'])\n",
    "                outputs = model(\n",
    "                    input_ids=masked_input,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=dec_input,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                amr_joint_loss2 = outputs[0]\n",
    "            else:\n",
    "                amr_joint_loss2 = 0\n",
    "\n",
    "            if args['mlm_joint_to_joint']:\n",
    "                mlm_prob = 0.35\n",
    "                masked_input, attention_mask, dec_input, labels = get_MTMG2TG(batch, tokenizer, mlm_prob=mlm_prob)\n",
    "                masked_input = masked_input.to(args['device'])\n",
    "                labels = labels.to(args['device'])\n",
    "                dec_input = dec_input.to(args['device'])\n",
    "                outputs = model(\n",
    "                    input_ids=masked_input,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=dec_input,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                joint2joint_loss = outputs[0]\n",
    "            else:\n",
    "                joint2joint_loss = 0\n",
    "\n",
    "            loss = amr_loss + text_loss + text_joint_loss + amr_joint_loss + text_joint_loss2 + amr_joint_loss2 + joint2joint_loss\n",
    "\n",
    "            pbar.set_postfix(lm_loss=loss.mean().item())\n",
    "\n",
    "            eval_loss += loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "    perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "\n",
    "    result = {\"perplexity\": perplexity, \"eval_loss\": eval_loss}\n",
    "\n",
    "    output_eval_file = os.path.join(eval_output_dir, prefix, \"eval_results.txt\")\n",
    "    with open(output_eval_file, \"a\") as writer:\n",
    "        for key in sorted(result.keys()):\n",
    "            print(f\"{key} = {result[key]}\")\n",
    "            writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "global_step = 0\n",
    "epochs_trained = 0\n",
    "epoch_step = 0\n",
    "steps_trained_in_current_epoch = 0\n",
    "best_score = float(\"inf\")\n",
    "\n",
    "tr_loss, logging_loss, epoch_loss = 0.0, 0.0, 0.0\n",
    "model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch:   0%|          | 0/13 [13:06<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/AIHCM/KGQA/NLPCore/graph2text/AMRBART/pre-train/pre-train.ipynb Cell 27'\u001b[0m in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/KGQA/NLPCore/graph2text/AMRBART/pre-train/pre-train.ipynb#ch0000024vscode-remote?line=29'>30</a>\u001b[0m     \u001b[39mif\u001b[39;00m step \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m epoch \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/KGQA/NLPCore/graph2text/AMRBART/pre-train/pre-train.ipynb#ch0000024vscode-remote?line=30'>31</a>\u001b[0m         save_dummy_batch2(args[\u001b[39m'\u001b[39m\u001b[39moutput_dir\u001b[39m\u001b[39m'\u001b[39m], masked_input, dec_input, labels, tokenizer, prefix\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mEtextamr2amr\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m---> <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/KGQA/NLPCore/graph2text/AMRBART/pre-train/pre-train.ipynb#ch0000024vscode-remote?line=31'>32</a>\u001b[0m     outputs \u001b[39m=\u001b[39m model(\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/KGQA/NLPCore/graph2text/AMRBART/pre-train/pre-train.ipynb#ch0000024vscode-remote?line=32'>33</a>\u001b[0m         input_ids\u001b[39m=\u001b[39;49mmasked_input,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/KGQA/NLPCore/graph2text/AMRBART/pre-train/pre-train.ipynb#ch0000024vscode-remote?line=33'>34</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/KGQA/NLPCore/graph2text/AMRBART/pre-train/pre-train.ipynb#ch0000024vscode-remote?line=34'>35</a>\u001b[0m         decoder_input_ids\u001b[39m=\u001b[39;49mdec_input,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/KGQA/NLPCore/graph2text/AMRBART/pre-train/pre-train.ipynb#ch0000024vscode-remote?line=35'>36</a>\u001b[0m         labels\u001b[39m=\u001b[39;49mlabels,\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/KGQA/NLPCore/graph2text/AMRBART/pre-train/pre-train.ipynb#ch0000024vscode-remote?line=36'>37</a>\u001b[0m     )\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/KGQA/NLPCore/graph2text/AMRBART/pre-train/pre-train.ipynb#ch0000024vscode-remote?line=37'>38</a>\u001b[0m     amr_loss \u001b[39m=\u001b[39m outputs[\u001b[39m0\u001b[39m]  \u001b[39m# model outputs are always tuple in transformers\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell://ssh-remote%2B172.28.0.23/AIHCM/KGQA/NLPCore/graph2text/AMRBART/pre-train/pre-train.ipynb#ch0000024vscode-remote?line=38'>39</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m/AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=886'>887</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=887'>888</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=888'>889</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=889'>890</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=890'>891</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=891'>892</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=892'>893</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py:1341\u001b[0m, in \u001b[0;36mMBartForConditionalGeneration.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, labels, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1337'>1338</a>\u001b[0m     \u001b[39mif\u001b[39;00m decoder_input_ids \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1338'>1339</a>\u001b[0m         decoder_input_ids \u001b[39m=\u001b[39m shift_tokens_right(labels, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mpad_token_id)\n\u001b[0;32m-> <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1340'>1341</a>\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel(\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1341'>1342</a>\u001b[0m     input_ids,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1342'>1343</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1343'>1344</a>\u001b[0m     decoder_input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1344'>1345</a>\u001b[0m     encoder_outputs\u001b[39m=\u001b[39;49mencoder_outputs,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1345'>1346</a>\u001b[0m     decoder_attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1346'>1347</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1347'>1348</a>\u001b[0m     decoder_head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1348'>1349</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1349'>1350</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1350'>1351</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1351'>1352</a>\u001b[0m     decoder_inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1352'>1353</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1353'>1354</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1354'>1355</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1355'>1356</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1356'>1357</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1357'>1358</a>\u001b[0m lm_logits \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlm_head(outputs[\u001b[39m0\u001b[39m]) \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfinal_logits_bias\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1359'>1360</a>\u001b[0m masked_lm_loss \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m/AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=886'>887</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=887'>888</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=888'>889</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=889'>890</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=890'>891</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=891'>892</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=892'>893</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py:1224\u001b[0m, in \u001b[0;36mMBartModel.forward\u001b[0;34m(self, input_ids, attention_mask, decoder_input_ids, decoder_attention_mask, head_mask, decoder_head_mask, cross_attn_head_mask, encoder_outputs, past_key_values, inputs_embeds, decoder_inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1216'>1217</a>\u001b[0m     encoder_outputs \u001b[39m=\u001b[39m BaseModelOutput(\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1217'>1218</a>\u001b[0m         last_hidden_state\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m0\u001b[39m],\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1218'>1219</a>\u001b[0m         hidden_states\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m1\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1219'>1220</a>\u001b[0m         attentions\u001b[39m=\u001b[39mencoder_outputs[\u001b[39m2\u001b[39m] \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(encoder_outputs) \u001b[39m>\u001b[39m \u001b[39m2\u001b[39m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1220'>1221</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1222'>1223</a>\u001b[0m \u001b[39m# decoder outputs consists of (dec_features, past_key_value, dec_hidden, dec_attn)\u001b[39;00m\n\u001b[0;32m-> <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1223'>1224</a>\u001b[0m decoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdecoder(\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1224'>1225</a>\u001b[0m     input_ids\u001b[39m=\u001b[39;49mdecoder_input_ids,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1225'>1226</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mdecoder_attention_mask,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1226'>1227</a>\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_outputs[\u001b[39m0\u001b[39;49m],\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1227'>1228</a>\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1228'>1229</a>\u001b[0m     head_mask\u001b[39m=\u001b[39;49mdecoder_head_mask,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1229'>1230</a>\u001b[0m     cross_attn_head_mask\u001b[39m=\u001b[39;49mcross_attn_head_mask,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1230'>1231</a>\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1231'>1232</a>\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49mdecoder_inputs_embeds,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1232'>1233</a>\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1233'>1234</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1234'>1235</a>\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1235'>1236</a>\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1236'>1237</a>\u001b[0m )\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1238'>1239</a>\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m return_dict:\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1239'>1240</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m decoder_outputs \u001b[39m+\u001b[39m encoder_outputs\n",
      "File \u001b[0;32m/AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=886'>887</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=887'>888</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=888'>889</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=889'>890</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=890'>891</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=891'>892</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=892'>893</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py:1090\u001b[0m, in \u001b[0;36mMBartDecoder.forward\u001b[0;34m(self, input_ids, attention_mask, encoder_hidden_states, encoder_attention_mask, head_mask, cross_attn_head_mask, past_key_values, inputs_embeds, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1077'>1078</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1078'>1079</a>\u001b[0m         create_custom_forward(decoder_layer),\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1079'>1080</a>\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1085'>1086</a>\u001b[0m         \u001b[39mNone\u001b[39;00m,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1086'>1087</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1087'>1088</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1089'>1090</a>\u001b[0m     layer_outputs \u001b[39m=\u001b[39m decoder_layer(\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1090'>1091</a>\u001b[0m         hidden_states,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1091'>1092</a>\u001b[0m         attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1092'>1093</a>\u001b[0m         encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1093'>1094</a>\u001b[0m         encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1094'>1095</a>\u001b[0m         layer_head_mask\u001b[39m=\u001b[39;49m(head_mask[idx] \u001b[39mif\u001b[39;49;00m head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m),\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1095'>1096</a>\u001b[0m         cross_attn_layer_head_mask\u001b[39m=\u001b[39;49m(\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1096'>1097</a>\u001b[0m             cross_attn_head_mask[idx] \u001b[39mif\u001b[39;49;00m cross_attn_head_mask \u001b[39mis\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1097'>1098</a>\u001b[0m         ),\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1098'>1099</a>\u001b[0m         past_key_value\u001b[39m=\u001b[39;49mpast_key_value,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1099'>1100</a>\u001b[0m         output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1100'>1101</a>\u001b[0m         use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1101'>1102</a>\u001b[0m     )\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1102'>1103</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[1;32m   <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=1104'>1105</a>\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
      "File \u001b[0;32m/AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=886'>887</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=887'>888</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=888'>889</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=889'>890</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=890'>891</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=891'>892</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=892'>893</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py:442\u001b[0m, in \u001b[0;36mMBartDecoderLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, encoder_hidden_states, encoder_attention_mask, layer_head_mask, cross_attn_layer_head_mask, past_key_value, output_attentions, use_cache)\u001b[0m\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=439'>440</a>\u001b[0m \u001b[39m# cross_attn cached key/values tuple is at positions 3,4 of present_key_value tuple\u001b[39;00m\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=440'>441</a>\u001b[0m cross_attn_past_key_value \u001b[39m=\u001b[39m past_key_value[\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m:] \u001b[39mif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m--> <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=441'>442</a>\u001b[0m hidden_states, cross_attn_weights, cross_attn_present_key_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder_attn(\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=442'>443</a>\u001b[0m     hidden_states\u001b[39m=\u001b[39;49mhidden_states,\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=443'>444</a>\u001b[0m     key_value_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=444'>445</a>\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mencoder_attention_mask,\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=445'>446</a>\u001b[0m     layer_head_mask\u001b[39m=\u001b[39;49mcross_attn_layer_head_mask,\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=446'>447</a>\u001b[0m     past_key_value\u001b[39m=\u001b[39;49mcross_attn_past_key_value,\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=447'>448</a>\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=448'>449</a>\u001b[0m )\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=449'>450</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m nn\u001b[39m.\u001b[39mfunctional\u001b[39m.\u001b[39mdropout(hidden_states, p\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout, training\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining)\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=450'>451</a>\u001b[0m hidden_states \u001b[39m=\u001b[39m residual \u001b[39m+\u001b[39m hidden_states\n",
      "File \u001b[0;32m/AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py:889\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=886'>887</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_slow_forward(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=887'>888</a>\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=888'>889</a>\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mforward(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=889'>890</a>\u001b[0m \u001b[39mfor\u001b[39;00m hook \u001b[39min\u001b[39;00m itertools\u001b[39m.\u001b[39mchain(\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=890'>891</a>\u001b[0m         _global_forward_hooks\u001b[39m.\u001b[39mvalues(),\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=891'>892</a>\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks\u001b[39m.\u001b[39mvalues()):\n\u001b[1;32m    <a href='file:///AIHCM/tmp/anaconda3/envs/amrb/lib/python3.8/site-packages/torch/nn/modules/module.py?line=892'>893</a>\u001b[0m     hook_result \u001b[39m=\u001b[39m hook(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m, result)\n",
      "File \u001b[0;32m/AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py:207\u001b[0m, in \u001b[0;36mMBartAttention.forward\u001b[0;34m(self, hidden_states, key_value_states, past_key_value, attention_mask, layer_head_mask, output_attentions)\u001b[0m\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=203'>204</a>\u001b[0m \u001b[39melif\u001b[39;00m is_cross_attention:\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=204'>205</a>\u001b[0m     \u001b[39m# cross_attentions\u001b[39;00m\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=205'>206</a>\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(key_value_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n\u001b[0;32m--> <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=206'>207</a>\u001b[0m     value_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_shape(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mv_proj(key_value_states), \u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m, bsz)\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=207'>208</a>\u001b[0m \u001b[39melif\u001b[39;00m past_key_value \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=208'>209</a>\u001b[0m     \u001b[39m# reuse k, v, self_attention\u001b[39;00m\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=209'>210</a>\u001b[0m     key_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_shape(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mk_proj(hidden_states), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, bsz)\n",
      "File \u001b[0;32m/AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py:178\u001b[0m, in \u001b[0;36mMBartAttention._shape\u001b[0;34m(self, tensor, seq_len, bsz)\u001b[0m\n\u001b[1;32m    <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=176'>177</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_shape\u001b[39m(\u001b[39mself\u001b[39m, tensor: torch\u001b[39m.\u001b[39mTensor, seq_len: \u001b[39mint\u001b[39m, bsz: \u001b[39mint\u001b[39m):\n\u001b[0;32m--> <a href='file:///AIHCM/KGQA/NLPCore/graph2text/transformers/src/transformers/models/mbart/modeling_mbart.py?line=177'>178</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m tensor\u001b[39m.\u001b[39;49mview(bsz, seq_len, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_heads, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mhead_dim)\u001b[39m.\u001b[39;49mtranspose(\u001b[39m1\u001b[39;49m, \u001b[39m2\u001b[39;49m)\u001b[39m.\u001b[39;49mcontiguous()\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "train_iterator = trange(\n",
    "    epochs_trained,\n",
    "    int(num_train_epochs),\n",
    "    desc=\"Epoch\",\n",
    "    disable=False\n",
    ")\n",
    "\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        train_dataloader, \n",
    "        desc=\"Iteration\", \n",
    "        disable=True\n",
    "    )\n",
    "\n",
    "    for step, batch in enumerate(epoch_iterator):\n",
    "        if steps_trained_in_current_epoch > 0:\n",
    "            steps_trained_in_current_epoch -= 1\n",
    "            continue\n",
    "        \n",
    "        model.train()\n",
    "\n",
    "        if args['mlm_amr']:     # [Empty text + Masked Graph -> graph]\n",
    "            masked_input, attention_mask, dec_input, labels = get_ETMG2graph(batch, tokenizer, mlm_prob=0.35) \n",
    "\n",
    "            masked_input = masked_input.to(args['device'])\n",
    "            attention_mask = attention_mask.to(args['device'])\n",
    "            labels = labels.to(args['device'])\n",
    "            dec_input = dec_input.to(args['device'])\n",
    "\n",
    "            if step == 0 and epoch == 0:\n",
    "                save_dummy_batch2(args['output_dir'], masked_input, dec_input, labels, tokenizer, prefix=\"Etextamr2amr\")\n",
    "            outputs = model(\n",
    "                input_ids=masked_input,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=dec_input,\n",
    "                labels=labels,\n",
    "            )\n",
    "            amr_loss = outputs[0]  # model outputs are always tuple in transformers\n",
    "        else:\n",
    "            amr_loss = 0\n",
    "        \n",
    "        if args['mlm_text']:    # [Masked Text + Empty Graph -> text]\n",
    "            masked_input, attention_mask, dec_input, labels = get_MTEG2text(batch, tokenizer, mlm_prob=0.35)\n",
    "\n",
    "            masked_input = masked_input.to(args['device'])\n",
    "            attention_mask = attention_mask.to(args['device'])\n",
    "            labels = labels.to(args['device'])\n",
    "            dec_input = dec_input.to(args['device'])\n",
    "            if step == 0 and epoch == 0:\n",
    "                save_dummy_batch2(args['output_dir'], masked_input, dec_input, labels, tokenizer, prefix=\"textEamr2text\")\n",
    "            outputs = model(\n",
    "                input_ids=masked_input,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=dec_input,\n",
    "                labels=labels,\n",
    "            )\n",
    "            text_loss = outputs[0]  # model outputs are always tuple in transformers (see doc)\n",
    "        else:\n",
    "            text_loss = 0\n",
    "        \n",
    "        if args['mlm_text_plus_amr']:   # [Masked text + Graph -> Text] \n",
    "            if step % args['joint_train_interval'] == 0:\n",
    "                mlm_prob = 0.1 + global_step / args['max_steps'] * 0.75\n",
    "                masked_input, attention_mask, dec_input, labels = get_PTPG2partial(batch, tokenizer, inp=\"text\", mlm_prob=mlm_prob)\n",
    "                masked_input = masked_input.to(args['device'])\n",
    "                attention_mask = attention_mask.to(args['device'])\n",
    "                labels = labels.to(args['device'])\n",
    "                dec_input = dec_input.to(args['device'])\n",
    "\n",
    "                if step == 0 and epoch == 0:\n",
    "                    save_dummy_batch2(args['output_dir'], masked_input, dec_input, labels, tokenizer, prefix=\"val_MtextAmr2text\")\n",
    "                outputs = model(\n",
    "                    input_ids=masked_input,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=dec_input,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                text_joint_loss = outputs[0]\n",
    "            else:\n",
    "                text_joint_loss = 0\n",
    "        else:\n",
    "            text_joint_loss = 0\n",
    "\n",
    "        if args['mlm_amr_plus_text']:   # [Text + Masked Graph -> Graph]\n",
    "            if step % args['joint_train_interval'] == 0:\n",
    "                mlm_prob = 0.1 + global_step / args['max_steps'] * 0.75\n",
    "                masked_input, attention_mask, dec_input, labels = get_PTPG2partial(batch, tokenizer, inp=\"amr\", mlm_prob=mlm_prob)\n",
    "                masked_input = masked_input.to(args['device'])\n",
    "                attention_mask = attention_mask.to(args['device'])\n",
    "                labels = labels.to(args['device'])\n",
    "                dec_input = dec_input.to(args['device'])\n",
    "                if step == 0 and epoch == 0:\n",
    "                    save_dummy_batch2(args['output_dir'],masked_input,dec_input,labels,tokenizer,prefix=\"val_TextMamr2amr\",)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=masked_input,\n",
    "                    attention_mask=attention_mask,\n",
    "                    decoder_input_ids=dec_input,\n",
    "                    labels=labels,\n",
    "                )\n",
    "                amr_joint_loss = outputs[0]\n",
    "            else:\n",
    "                amr_joint_loss = 0\n",
    "        else:\n",
    "            amr_joint_loss = 0\n",
    "\n",
    "        if args['mlm_joint_to_text']:   # [Masked Text + Masked Graph -> text]\n",
    "            mlm_prob = 0.35\n",
    "            masked_input, attention_mask, dec_input, labels = get_MTMG2partial(batch, tokenizer, inp=\"text\", mlm_prob=mlm_prob)\n",
    "            masked_input = masked_input.to(args['device'])\n",
    "            attention_mask = attention_mask.to(args['device'])\n",
    "            labels = labels.to(args['device'])\n",
    "            dec_input = dec_input.to(args['device'])\n",
    "            if step == 0 and epoch == 0:\n",
    "                save_dummy_batch2(args['output_dir'],masked_input,dec_input,labels,tokenizer,prefix=\"val_MtextMamr2text\",)\n",
    "            outputs = model(\n",
    "                input_ids=masked_input,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=dec_input,\n",
    "                labels=labels,\n",
    "            )\n",
    "            text_joint_loss2 = outputs[0]\n",
    "        else:\n",
    "            text_joint_loss2 = 0\n",
    "\n",
    "        if args['mlm_joint_to_amr']:    # [Masked Text + Masked Graph -> graph]\n",
    "            mlm_prob = 0.35\n",
    "            masked_input, attention_mask, dec_input, labels = get_MTMG2partial(batch, tokenizer, inp=\"amr\", mlm_prob=mlm_prob)\n",
    "            masked_input = masked_input.to(args['device'])\n",
    "            attention_mask = attention_mask.to(args['device'])\n",
    "            labels = labels.to(args['device'])\n",
    "            dec_input = dec_input.to(args['device'])\n",
    "            if step == 0 and epoch == 0:\n",
    "                save_dummy_batch2(args['output_dir'], masked_input, dec_input, labels, tokenizer, prefix=\"val_MtextMamr2amr\")\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=masked_input,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=dec_input,\n",
    "                labels=labels,\n",
    "            )\n",
    "            amr_joint_loss2 = outputs[0]\n",
    "        else:\n",
    "            amr_joint_loss2 = 0\n",
    "\n",
    "        if args['mlm_joint_to_joint']:\n",
    "            mlm_prob = 0.35\n",
    "            masked_input, attention_mask, dec_input, labels = get_MTMG2TG(batch, tokenizer, mlm_prob=mlm_prob)\n",
    "            masked_input = masked_input.to(args['device'])\n",
    "            attention_mask = attention_mask.to(args['device'])\n",
    "            attention_mask = attention_mask.to(args['device'])\n",
    "            labels = labels.to(args['device'])\n",
    "            dec_input = dec_input.to(args['device'])\n",
    "            if step == 0 and epoch == 0:\n",
    "                save_dummy_batch2(args['output_dir'],masked_input,dec_input,labels,tokenizer,prefix=\"val_MtextMamr2textamr\",)\n",
    "\n",
    "            outputs = model(\n",
    "                input_ids=masked_input,\n",
    "                attention_mask=attention_mask,\n",
    "                decoder_input_ids=dec_input,\n",
    "                labels=labels,\n",
    "            )\n",
    "            joint2joint_loss = outputs[0]\n",
    "        else:\n",
    "            joint2joint_loss = 0\n",
    "        \n",
    "        loss = amr_loss + text_loss+ text_joint_loss+ amr_joint_loss+ text_joint_loss2+ amr_joint_loss2+ joint2joint_loss\n",
    "        loss = loss / args['gradient_accumulation_steps']\n",
    "\n",
    "        # epoch_iterator.set_postfix(lm_loss=loss.item(), lr=scheduler.get_lr()[0])\n",
    "        epoch_iterator.set_postfix(lm_loss=loss.item(), lr=scheduler.get_last_lr()[0])\n",
    "\n",
    "        if args['fp16']:\n",
    "            with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                scaled_loss.backward()\n",
    "        else:\n",
    "            loss.backward()\n",
    "\n",
    "        epoch_step += 1\n",
    "        tr_loss += loss.item()\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "        if (step + 1) % args['gradient_accumulation_steps'] == 0:\n",
    "            if args['fp16']:\n",
    "                torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args['max_grad_norm'])\n",
    "            else:\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), args['max_grad_norm'])\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()  # Update learning rate schedule\n",
    "            model.zero_grad()\n",
    "            global_step += 1\n",
    "\n",
    "            if (args['logging_steps'] > 0 and global_step % args['logging_steps'] == 0):\n",
    "\n",
    "                if args['evaluate_during_training']:\n",
    "                    results = evaluate(args, dev_dataset, seq2seq_collate_fn, model, tokenizer)\n",
    "                    cur_score = results['perplexity'].item()\n",
    "\n",
    "                    if cur_score < best_score:\n",
    "                        best_score = cur_score\n",
    "                        checkpoint_prefix = \"checkpoint\"\n",
    "                        # Save model checkpoint\n",
    "                        output_dir = os.path.join(\n",
    "                            args['output_dir'],\n",
    "                            \"{}-{}-{:.3f}\".format(checkpoint_prefix, global_step, best_score),\n",
    "                        )\n",
    "                        os.makedirs(output_dir, exist_ok=True)\n",
    "                        model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n",
    "                        model_to_save.save_pretrained(output_dir)\n",
    "                        # tokenizer.save_pretrained(output_dir)\n",
    "                        with open(f'{epoch_output_dir}/tokenizer.pickle', 'wb') as handle:\n",
    "                            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "                        torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "                        print(f\"Saving model checkpoint to {output_dir}\")\n",
    "\n",
    "                        _rotate_checkpoints(args, checkpoint_prefix)\n",
    "\n",
    "                        torch.save(optimizer.state_dict(), os.path.join(output_dir, \"optimizer.pt\"))\n",
    "                        torch.save(scheduler.state_dict(), os.path.join(output_dir, \"scheduler.pt\"))\n",
    "                        print(f\"Saving optimizer and scheduler states to {output_dir}\")\n",
    "\n",
    "                logging_loss = tr_loss\n",
    "\n",
    "        if args['max_steps'] > 0 and global_step > args['max_steps']:\n",
    "            epoch_iterator.close()\n",
    "            break\n",
    "    \n",
    "    if args['max_steps'] > 0 and global_step > args['max_steps']:\n",
    "        results = evaluate(args, dev_dataset, seq2seq_collate_fn, model, tokenizer)\n",
    "        cur_score = results[\"perplexity\"].item()\n",
    "        checkpoint_prefix = \"checkpoint\"\n",
    "        # Save model checkpoint\n",
    "        ckpt_output_dir = os.path.join(args['output_dir'], \"{}-last-{:.3f}\".format(checkpoint_prefix, cur_score))\n",
    "        os.makedirs(ckpt_output_dir, exist_ok=True)\n",
    "        model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n",
    "        model_to_save.save_pretrained(ckpt_output_dir)\n",
    "        # tokenizer.save_pretrained(ckpt_output_dir)\n",
    "        with open(f'{epoch_output_dir}/tokenizer.pickle', 'wb') as handle:\n",
    "            pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Saving model checkpoint to {ckpt_output_dir}\")\n",
    "        train_iterator.close()\n",
    "        break\n",
    "\n",
    "    checkpoint_prefix = \"checkpoint\"\n",
    "    epoch_output_dir = os.path.join(args['output_dir'], \"{}-last-epoch\".format(checkpoint_prefix),)\n",
    "    os.makedirs(epoch_output_dir, exist_ok=True)\n",
    "    model_to_save = (model.module if hasattr(model, \"module\") else model)  # Take care of distributed/parallel training\n",
    "    model_to_save.save_pretrained(epoch_output_dir)\n",
    "    # tokenizer.save_pretrained(epoch_output_dir)\n",
    "    with open(f'{epoch_output_dir}/tokenizer.pickle', 'wb') as handle:\n",
    "        pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    print(f\"Saving model checkpoint to {epoch_output_dir}\")\n",
    "    avg_epoch_loss = epoch_loss / epoch_step\n",
    "    print(f'avg_train_loss = {avg_epoch_loss}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "5eb764fc1274c4307015b9b041006d02a02c66de6b3042b9eea4c0fcfdfe1065"
  },
  "kernelspec": {
   "display_name": "Python 3.8.13 ('hoangtv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
